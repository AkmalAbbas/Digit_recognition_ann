# -*- coding: utf-8 -*-
"""Experiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PD2qmcnF7zqeqOiFDrb41wPBgl98KgvE
"""
# Syed Akmal Abbas 01-249192-016
from google.colab import files
uploaded = files.upload()

!unzip '/content/train.zip'
!unzip '/content/valid.zip'

import glob
from PIL import Image
import os
import numpy as np
# Importing Keras
import tensorflow
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from sklearn.model_selection import KFold

# Commented out IPython magic to ensure Python compatibility.
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
# %matplotlib inline

# Seed value
# Apparently you may use different seed values at each stage
seed_value= 0

# 1. Set `PYTHONHASHSEED` environment variable at a fixed value
import os
os.environ['PYTHONHASHSEED']=str(seed_value)

# 2. Set `python` built-in pseudo-random generator at a fixed value
import random
random.seed(seed_value)

# 3. Set `numpy` pseudo-random generator at a fixed value
np.random.seed(seed_value)

# 4. Set the `tensorflow` pseudo-random generator at a fixed value
tensorflow.random.set_seed(seed_value)
# for later versions: 
# tf.compat.v1.set_random_seed(seed_value)

# 5. Configure a new global `tensorflow` session
from tensorflow.keras import backend as K
session_conf = tensorflow.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
sess = tensorflow.compat.v1.Session(graph=tensorflow.compat.v1.get_default_graph(), config=session_conf)
tensorflow.compat.v1.keras.backend.set_session(sess)


# Functions to load labels of digits
def load_labels(myDir):
    labels=[]
    fileList = glob.glob(myDir)
    for fname in fileList:
        fileName = os.path.basename(fname)
        curLabel = fileName.split("-")[0]
        labels.append(curLabel)
    return np.asarray(labels)

# Function to Load Image data and then normalize it 
def load_data(myDir):
    fileList = glob.glob(myDir)    
    x = np.array([np.array(Image.open(fname)).flatten() for fname in fileList])
    x=x/255
    return x

myDir ="/content/train/*.png"
labels = load_labels(myDir)
data = load_data(myDir)
Labels = tensorflow.keras.utils.to_categorical(labels,10)

# Splitting the dataset into Training and Test set
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(data,Labels,test_size=0.3,random_state=42)

# Function for experiment with single hidden layer
result = {}
model_collection = {}
def multi_layer_perceptron(num_of_nodes,activation_function,dropout_rate):
  for activation in activation_function:
    for nodes in num_of_nodes:
      for dropout in dropout_rate:
        print(f"\nMLP architecture Total Nodes : {nodes} Activation : {activation} and Dropout : {dropout}")
        classifier = Sequential()
        classifier.add(Dense(units=nodes,activation=activation,kernel_initializer='glorot_uniform',input_dim=784)) 
        if dropout != 0.0:
          classifier.add(Dropout(dropout))     
        classifier.add(Dense(10,activation='softmax'))
        #optimizer = tensorflow.keras.optimizers.Adamax(learning_rate=0.01)
        classifier.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
        history = classifier.fit(X_train,y_train,validation_data=(X_test, y_test) , epochs=100 , batch_size=20 , verbose=0)
        key = 'activation ' + activation + ' Total Nodes '+ str(nodes) + ' Dropout '+ str(dropout) 
        model_collection[key] = classifier
        result[key] = history      
        print(f"\nMLP trained on {nodes} and {activation}")
  return result

nodes = [230 ,330 , 430 , 530 , 630 , 730 , 830]
activations = ['relu' , 'sigmoid', 'tanh']
dropout_value = [0.0 , 0.1 , 0.2 , 0.3 , 0.4]
score_history = multi_layer_perceptron(num_of_nodes=nodes,activation_function=activations,dropout_rate = dropout_value)

# Print accuracy score and loss for every experiment
for i,hist in score_history.items():
  print(f"----- Summary for MLP {i} ---------")
  print(f"Training set accuracy : {hist.history['accuracy'][-1]}")
  print(f"Validation set accuracy : {hist.history['val_accuracy'][-1]}")
  print(f"Training set Loss : {hist.history['loss'][-1]}")
  print(f"Validation set Loss : {hist.history['val_loss'][-1]}")
  print("-------------------------------------")

# Print accuracy on unseen validation set for experiments
myDir ="/content/valid/*.png"
labels_val = load_labels(myDir)
data_val = load_data(myDir)
Labels_val = tensorflow.keras.utils.to_categorical(labels_val,10)
for i,model in model_collection.items():
  scores = model.evaluate(data_val,Labels_val)
  print(f"----- Score summary on validation model MLP {i} ---------")
  print(f"Training set accuracy : {scores}")
  print("-------------------------------------")

# Plotting the learning and loss curves
for i,hist in score_history.items():
  # summarize history for accuracy
  plt.plot(hist.history['accuracy'])
  plt.plot(hist.history['val_accuracy'])
  plt.title('model accuracy for '+ i)
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()

  plt.plot(hist.history['loss'])
  plt.plot(hist.history['val_loss'])
  plt.title('model loss for '+ i)
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()

######################################################
## Experment with 2 hidden layers
classifier = Sequential()
classifier.add(Dense(units=230,activation='sigmoid',kernel_initializer='glorot_uniform',input_dim=784))   
classifier.add(Dropout(0.3))   
classifier.add(Dense(units=115,activation='sigmoid'))    
classifier.add(Dropout(0.3))
classifier.add(Dense(10,activation='softmax'))
classifier.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
history = classifier.fit(X_train,y_train,validation_data=(X_test, y_test) , epochs=100 , batch_size=20 , verbose=1)

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss 2 Layers 230 Nodes')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

classifier_2Layer_530 = Sequential()
classifier_2Layer_530.add(Dense(units=530,activation='sigmoid',kernel_initializer='glorot_uniform',input_dim=784))    
classifier_2Layer_530.add(Dropout(0.3))   
classifier_2Layer_530.add(Dense(units=530,activation='sigmoid'))    
classifier_2Layer_530.add(Dropout(0.3))
classifier_2Layer_530.add(Dense(10,activation='softmax'))
classifier_2Layer_530.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
history_2Layer_530 = classifier_2Layer_530.fit(X_train,y_train,validation_data=(X_test, y_test) , epochs=100 , batch_size=20 , verbose=1)

# summarize history for accuracy
plt.plot(history_2Layer_530.history['accuracy'])
plt.plot(history_2Layer_530.history['val_accuracy'])
plt.title('model accuracy 2 Layers [530,530] Nodes')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history_2Layer_530.history['loss'])
plt.plot(history_2Layer_530.history['val_loss'])
plt.title('model loss 2 Layers [530,530] Nodes')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

classifier_2Layer_530 = Sequential()
classifier_2Layer_530.add(Dense(units=530,activation='sigmoid',kernel_initializer='glorot_uniform',input_dim=784))    
classifier_2Layer_530.add(Dropout(0.3))   
classifier_2Layer_530.add(Dense(units=430,activation='sigmoid'))    
classifier_2Layer_530.add(Dropout(0.3))
classifier_2Layer_530.add(Dense(10,activation='softmax'))
classifier_2Layer_530.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
history_2Layer_530 = classifier_2Layer_530.fit(X_train,y_train,validation_data=(X_test, y_test) , epochs=100 , batch_size=20 , verbose=1)

# summarize history for accuracy
plt.plot(history_2Layer_530.history['accuracy'])
plt.plot(history_2Layer_530.history['val_accuracy'])
plt.title('model accuracy 2 Layers [530,430] Nodes')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history_2Layer_530.history['loss'])
plt.plot(history_2Layer_530.history['val_loss'])
plt.title('model loss 2 Layers [530,430] Nodes')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

classifier_2Layer_530 = Sequential()
classifier_2Layer_530.add(Dense(units=530,activation='sigmoid',kernel_initializer='glorot_uniform',input_dim=784))    
classifier_2Layer_530.add(Dropout(0.3))   
classifier_2Layer_530.add(Dense(units=330,activation='sigmoid'))    
classifier_2Layer_530.add(Dropout(0.3))
classifier_2Layer_530.add(Dense(10,activation='softmax'))
classifier_2Layer_530.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
history_2Layer_530 = classifier_2Layer_530.fit(X_train,y_train,validation_data=(X_test, y_test) , epochs=100 , batch_size=20 , verbose=1)

# summarize history for accuracy
plt.plot(history_2Layer_530.history['accuracy'])
plt.plot(history_2Layer_530.history['val_accuracy'])
plt.title('model accuracy 2 Layers [530,330] Nodes')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history_2Layer_530.history['loss'])
plt.plot(history_2Layer_530.history['val_loss'])
plt.title('model loss 2 Layers [530,330] Nodes')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

classifier_2Layer_530 = Sequential()
classifier_2Layer_530.add(Dense(units=530,activation='sigmoid',kernel_initializer='glorot_uniform',input_dim=784))    
classifier_2Layer_530.add(Dropout(0.3))   
classifier_2Layer_530.add(Dense(units=230,activation='sigmoid'))    
classifier_2Layer_530.add(Dropout(0.3))
classifier_2Layer_530.add(Dense(10,activation='softmax'))
classifier_2Layer_530.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
history_2Layer_530 = classifier_2Layer_530.fit(X_train,y_train,validation_data=(X_test, y_test) , epochs=100 , batch_size=20 , verbose=1)

# summarize history for accuracy
plt.plot(history_2Layer_530.history['accuracy'])
plt.plot(history_2Layer_530.history['val_accuracy'])
plt.title('model accuracy 2 Layers [530,230] Nodes')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history_2Layer_530.history['loss'])
plt.plot(history_2Layer_530.history['val_loss'])
plt.title('model loss 2 Layers [530,230] Nodes')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

classifier_2Layer_530 = Sequential()
classifier_2Layer_530.add(Dense(units=530,activation='sigmoid',kernel_initializer='glorot_uniform',input_dim=784))    
classifier_2Layer_530.add(Dropout(0.3))   
classifier_2Layer_530.add(Dense(units=130,activation='sigmoid'))    
classifier_2Layer_530.add(Dropout(0.3))
classifier_2Layer_530.add(Dense(10,activation='softmax'))
classifier_2Layer_530.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
history_2Layer_530 = classifier_2Layer_530.fit(X_train,y_train,validation_data=(X_test, y_test) , epochs=100 , batch_size=20 , verbose=1)

# summarize history for accuracy
plt.plot(history_2Layer_530.history['accuracy'])
plt.plot(history_2Layer_530.history['val_accuracy'])
plt.title('model accuracy 2 Layers [530,130] Nodes')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history_2Layer_530.history['loss'])
plt.plot(history_2Layer_530.history['val_loss'])
plt.title('model loss 2 Layers [530,130] Nodes')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

classifier_2Layer_530 = Sequential()
classifier_2Layer_530.add(Dense(units=530,activation='sigmoid',kernel_initializer='glorot_uniform',input_dim=784))    
classifier_2Layer_530.add(Dropout(0.3))   
classifier_2Layer_530.add(Dense(units=830,activation='sigmoid'))    
classifier_2Layer_530.add(Dropout(0.3))
classifier_2Layer_530.add(Dense(10,activation='softmax'))
classifier_2Layer_530.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
history_2Layer_530 = classifier_2Layer_530.fit(X_train,y_train,validation_data=(X_test, y_test) , epochs=100 , batch_size=20 , verbose=1)

# summarize history for accuracy
plt.plot(history_2Layer_530.history['accuracy'])
plt.plot(history_2Layer_530.history['val_accuracy'])
plt.title('model accuracy 2 Layers [530,130] Nodes')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history_2Layer_530.history['loss'])
plt.plot(history_2Layer_530.history['val_loss'])
plt.title('model loss 2 Layers [530,130] Nodes')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
#####################################################

## Experiment for batch size and epoch using GridSearchCV
def build_classifier():
  classifier = Sequential()
  classifier.add(Dense(units=530,activation='sigmoid',kernel_initializer='glorot_uniform',input_dim=784))
  classifier.add(Dropout(0.3))
  classifier.add(Dense(units=230,activation='sigmoid'))
  classifier.add(Dropout(0.3))
  classifier.add(Dense(10,activation='softmax'))  
  classifier.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
  return classifier

model = KerasClassifier(build_fn=build_classifier,verbose=0)

batch_size = [20, 50, 80, 100]
epochs = [10, 20 , 50, 100]
param_grid = dict(batch_size=batch_size, epochs=epochs)
grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)
grid_result = grid.fit(X_train, y_train)

# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

# Plotting accuracy bars for batch and epochs
x_nodes = ['20,10','20,20','20,50','20,100','50,10','50,20','50,50','50,100','80,10','80,20','80,50','80,100','100,10','100,20','100,50','100,100']
y_nodes = [0.7855,0.8124,0.8424,0.8848,0.7832,0.8002,0.8379,0.8804,0.7746,0.8059,0.8202,0.8771,0.7746,0.8126,0.8232,0.8779]
import matplotlib.pyplot as plt
#plt.figure(figsize=(10,4))
fig = plt.figure(figsize=(10,4))
ax = fig.add_axes([0,0,1,1])
#langs = ['C', 'C++', 'Java', 'Python', 'PHP']
#students = [23,17,35,29,12]
ax.bar(x_nodes,y_nodes,width = 0.50)
plt.show()

#####################################################
# Experiment with different optimizers
def build_classifier_optimizer(optimizer = 'adam'):
  classifier = Sequential()
  classifier.add(Dense(units=530,activation='sigmoid',kernel_initializer='glorot_uniform',input_dim=784))
  classifier.add(Dropout(0.3))
  classifier.add(Dense(units=230,activation='sigmoid'))
  classifier.add(Dropout(0.3))
  classifier.add(Dense(10,activation='softmax'))  
  classifier.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])
  return classifier

model = KerasClassifier(build_fn=build_classifier_optimizer, epochs=100, batch_size=20, verbose=0)
# define the grid search parameters
optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']
param_grid = dict(optimizer=optimizer)
grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)
grid_result = grid.fit(X_train, y_train)

# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

#Plotting the optimizer accuracy bars
x_nodes = ['SGD','RMSprop','Adagrad','Adadelta','Adam','Adamax','Nadam']
y_nodes = [0.764286 ,0.877553,0.630612,0.285119,0.883472,0.858779,0.883266]

import matplotlib.pyplot as plt
fig = plt.figure(figsize=(10,4))
ax = fig.add_axes([0,0,1,1])
ax.bar(x_nodes,y_nodes,width = 0.50)
plt.show()

#############################################
## Finalized Model
############################################
classifier = Sequential()
classifier.add(Dense(units=530,activation='sigmoid',kernel_initializer='glorot_uniform',input_dim=784))
classifier.add(Dropout(0.3))
classifier.add(Dense(units=230,activation='sigmoid'))
classifier.add(Dropout(0.3))
classifier.add(Dense(10,activation='softmax'))  
classifier.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
history= classifier.fit(data,Labels , epochs=100 , batch_size=20 , verbose=1)

from keras.utils.vis_utils import plot_model
plot_model(classifier, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

print(classifier.summary())

myDir_valid ="/content/valid/*.png"
labels_valid = load_labels(myDir_valid)
data_valid = load_data(myDir_valid)
Labels_valid = tensorflow.keras.utils.to_categorical(labels_valid,10)

y_pred = classifier.predict(data_valid)
print(y_pred.shape)
print(Labels_valid.shape)

###################################
# cross validation to check accuracy 
# and variance of the model 
# on different folds
def build_classifier_cv():
  classifier = Sequential()
  classifier.add(Dense(units=530,activation='sigmoid',kernel_initializer='glorot_uniform',input_dim=784))
  classifier.add(Dropout(0.3))
  classifier.add(Dense(units=230,activation='sigmoid'))
  classifier.add(Dropout(0.3))
  classifier.add(Dense(10,activation='softmax'))  
  classifier.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
  return classifier

model = KerasClassifier(build_fn=build_classifier_cv , batch_size=20 , epochs = 100 , verbose=0)
scores = cross_val_score(estimator=model , X = data , y = Labels , cv = 5)
mean_accuracy = scores.mean()
variance = scores.std()
print(mean_accuracy)
print(variance)

###########################33
# Checking the output
test_input = '/content/train/8-0192-20-01.png'
x = np.array([np.array(Image.open(test_input)).flatten()])
x=x/255
y_predict = classifier.predict(x)
y_predict.argmax()





